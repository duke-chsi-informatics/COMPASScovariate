---
title: "COMPASScovariate Test Simulations"
author: "Tyler Schappe"
date: "2022-11-22"
output: 
  html_document:
    code_folding: show
    toc: true
    toc_float: true
    toc_depth: 6
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Build COMPASScovariate package
devtools::load_all("..")

#Source for now but will want to call using COMPASS.R eventually
source("~/project_repos/COMPASScovariate/R/COMPASS-covariate.R")
source("~/project_repos/COMPASScovariate/R/updatebeta.R")
source("~/project_repos/COMPASScovariate/R/utils.R")

library(foreach); library(doParallel); library(HMP); library(MCMCpack); library(devtools); library(ggplot2); library(tidyverse); library(grid)

# install.packages("MultiRNG")
```

## Notes

**Note on iterations:** Keep only the last X iterations and use the first ones to warm up chains.

**12/13/22 To Do:**

- Create density plot of posterior for betas that have a large difference between estimated and true.
- Reduce to 2 markers = 4 cell subsets
- Modify covariates (intercept) to allow ~ 50% of gammas to be 1

### Additional Complexities

1) Smaller difference in concentration parameters between stim and unstim
2) Different number of total events and event subsets for stim and unstim samples
3) Smaller percentage of gamma = 1

**Notes from meeting on 12/21/22**

Two types of covariates -- patient characteristics and experimental design

Want some cell subsets to be 0 when covariate (age) 

HIV group (treatment group) -- more + gammas
Young people -- less + gammas
HIV group also have more young people so that without knowing (conditioning on age), there is no association with HIV group

Don't include HIV in COMPASScovariate

Treatment group (HIV): 

- Larger sigma.beta
- Larger cutoff value for beta selection
- Outcome is vector of length I -- binary
  + If gamma++ == 1, then outcome = 1; else outcome = 0


0. Remove all other covariates
1. Specify # of HIV+ and HIV-
1. Sim with age and HIV
  1. Age (X) | HIV+: Young --  rnorm(-1, 0.4) (plot values to see overlap)
  2. Age (X) | HIV-: Old --    rnorm(1, 0.4)
2. Run COMPASScovariate with just age
3. Run original COMPASS
4. Logistic regression to see if PFS scores are associated with HIV
  1. COMPASScovariate PFS -- hope to be associated with HIV
  2. COMPASS PFS -- hope to not be associated


**Notes from meeting on 1/24/23**

- Setup with age | HIV status makes sense, but we need to have the pattern emerge only when age is accounted for and have no pattern when it's not (something paradox)
- Increase the number of markers to 8 so that we don't have people randomly have none or all subsets stimulated
- Can check difference between mean.gamma for COMPASScovariate and COMPASS to identify subjects for which the model responds differently

**Notes from meeting on 1/31/23**

- Consider clinical outcome to be protection status, defined as not becoming infected when challenged
  + Assume protection comes from a robust immune response == high polyfunctional score
  + Probability of stimulation is a function of age
- The chain of causality is: Age -> Prob. stim -> PFS -> Protection
  + We created a set of DAGs that is uploaded as a PDF to /incubator/sim/DAG.pdf
  + In the DAG, Age is a classic confounder between PFS and protection
  + Practically speaking, we hope that COMPASS will give PFS such that the relationship with protection is weak, but COMPASScovariate will give improved PFS such that the relationship is stronger
- For simulating true PFS from Prob. stim, can use the PolyfunctionalScore() function but pass it the true Gamma matrix (it normally uses the estimated mean gamma matrix)
- For simulating true Protection status, need to choose a true beta and use a logistic regression-type link where PFS is associated with higher probability of Protection
- Lynn also wants to add more noise the signal of the magnitude of change in probability between stim and unstim, which would translate to drawing ps.logmu.diff from some distribution with a small amount of variance
- To do:
  + Add variance to drawing ps.logmu.diff
  + Add generating true PFS scores using true gamma matrix
  + Add true betas for relationship between PFS and probability of protection
  + Add generating true protection status from PFS given the beta

**Notes from meeting on 2/7/23**

- Modify concentration parameters so that the amount added (diff) is positively associated with age. Make it the same for all cell subsets. 

**Notes from meeting on 2/14/23:**

- COMPASScovariate cannot adjust for confounding/bias because of the within-subject application of the stimulation treatment -- since every subject is given both treatment (stim) and control (unstim), there is basically no way a covariate can affect which exposure each subject receives. 
  + Therefore, consider the covariate as an effect modifier -- accounting for it can improve the precision of the estimates
- For ease of interpretation, change continuous covariate to binary indicator.

**Notes from meeting on 2/21/23**

- Lynn agrees precision of estimates should be a focus for COMPASScovariate
- To do:
  + Check the acceptance rate for the sampling -- if too low, may need to tune the model
  + Compare mean gamma (estimates of probability of response) to true probabilities instead of comparing gamma. Find the differences for both COMPASS and COMPASScovariate, then find the difference in those differences to look for subject-cell types. 
  + Increase magnitude of stimulation betas -- draw them from uniform(1,2)

**Notes from meeting on 2/28/23**

- Concentration_s and concentration_u are the same for all subjects, fixed not drawn from log-normal (manually specify them)
- Do not use Dirichlet-Multinomial because can't normalize p_s
- Determine p_u first, then create p_s for gamma == 1 and normalize before adding back to vector. This allows for p_s and p_u to be the same value when gamma == 1 (required by COMPASS). Because of this, there has to be more than 1 gamma == 1 because of the constraint that we can't change a single p_s without changing the others in that case.

Code from Lynn to make Kth subset have much higher counts
alpha_u[1, 1:(K - 1)] = 10 #initializaion 
  alpha_u[1, K] = 150
  
  alpha_s[1, 1:(K - 1)] = 10 #initialization 
  alpha_s[1, K] = 100
  
**Toy example for normalization**

```{r eval=FALSE, include=FALSE}
gamma = c(0, 1, 1, 0,
          1, 0, 1, 0
)

alpha.u = c(10, 10, 10, 100)
alpha.s = c(50, 50, 50, 100)

alpha.s = X

p_u = rdirichlet(n = 1, alpha = alpha.u)

p_s = p_u

p_s[2:3] = rdirichlet(n = 1, alpha = alpha.s[2:3]) * sum(p_u[gamma == 1])

sum(p_s)
```

**Notes from meeting on 3/28/23**

- Add shift of concentration parameter depending on number of subsets that are stimulated for subject -- goal is to have weight for a single stimulated subset be the same as the weight of each of the subsets when many are stimulated in order to avoid giving too much weight to subsets when only a few are stimulated for a sample. Cliburn suggested thinking about it in terms of a counterfactual when the same subject has the covariate or does not have the covariate -- the weight given in the no covariate case to each subset shouldn't be a lot larger. 
- Examine traceplots for convergence -- does covariate help convergence for clusters where not many are responding
- Color mean gamma - mean gamma plots by true gamma value
- Reduce subjects to 40
- Add logistic regression with mean_gamma as covariates (first K-1 subsets) instead of using PFS

**Notes from meeting on 4/4/23:**

- Added tentative shift of concentration parameter, Lynn said it looks fine
- Try combinations of:
  + Reducing prevalence of binary covariate == 1 in sample
  + Reducing probability of response to stimulation when covariate == 1 (signal)
  + Doing both above
- Methods for comparing models
  + True gamma vs mean_gamma for each model
    + Percent correct
    + Percent over/under
  + Mean_gamma COMPASS vs mean_gamma COMPASScov
  + Convergence of samplers
  

## Simulate Data and Run COMPASScovariate

**Approach**

1) Matrix of true betas for each k-1 subset for each covariate p ~ Normal(beta.mu, beta.sigma)
    a) Model selection where any beta where |beta| < beta.selection.cutoff -> 0
2) Vector of true intercepts for each k-1 subset ~ Normal(int.mu, int.sd)
    a) int.mu is 0 to keep probability that gamma = 1 ~ 50%, which makes estimating betas easier.
3) Matrix X (I x p) of covariates with first column of 1's for intercept
4) Omega (I x K1): For each sample, for each subset, generate linear predictor as linear combination of sample's covariate values X[i,] and subset-specific true betas beta.true.final[k,]
5) gamma.m (I x K1): For each sample, for each subset, map linear predictor to probability using inverse-logit function
6) Generate unstimulated and stimulated counts from a Dirichlet-Multinomial distribution where the concentration parameters are drawn from a lognormal distribution with same logmean and logsd values.
    a) For stimulated cluster-samples (where gamma = 1), shift the logmean by ps.logmu.diff to increase the concentration parameter by exp(ps.logmu.diff).
    b) For each sample, number of stimulated and unstimulated counts ~ Normal(e.total.mu, e.total.sd). This is the total number of stim and unstim counts for each sample.
    c) For both stimluated and unstimulated, use Dirichlet-Multinomial to draw subset of stimulated and unstimulated counts ~ Normal(e.mu, e.sd). This is the number of stim and unstim counts for all K subsets. **Note:** The values of the concentration parameter can be interpreted as prior pseudo-counts and thus determine the variance among categories. Use high concentration parameter values to reduce the variance in counts among categories in resulting draws.
    d) **Important:** The Kth category is included in the draws, but has a very large concentration parameter compared to rest of the categories so that the number of counts is much larger. This is to mimic real flow data where the majority of cells are in the "all negative" subset (ie. they don't belong to any of the mutually exclusive categories created by the markers of interest). We include this category because otherwise additional counts for a stimulated cell subset take away counts from only the k - 1 markers, resulting in significantly reduced counts for them (Lynn's original method). Here, the counts for the stimulated cell subset are "taken" from all other subsets with probability proportionate to the concentration parameters. Since the kth subset is given such a high concentration, it is much more likely that the counts are "taken" from that category, thus allowing the counts for the k - 1 subsets to remain constant (with some noise) between unstim and stim samples if they are not truely stimulated. 
  

### Set Fixed Parameters

```{r}
set.seed(111)
#Get number of iter and reps from 'run_simu_cov.sh'
# iter = as.integer(Sys.getenv("ITER")) #Number of COMPASScovariate iterations per replicate
# rep = as.integer(Sys.getenv("REP"))   #Number of COMPASScovariate replicates
# sim.rep = as.integer(Sys.getenv("SIM_REP")) #Number of replicates per parameter combination

iter = 25000 #Number of COMPASScovariate iterations per replicate
rep = 2     #Number of COMPASScovariate replicates
sim.rep = 4

#Total counts
e.total.mu = 10000 #Mean total number of events per subject
e.total.sd = 10   #SD of total number of events per subject
e.subset.mu = 3000 #Mean number of events per subject that are not in the Kth category (all negative set)
e.subset.sd = 5   #SD of number of events per subject that are not in the Kth category (all negative set)

#Betas
beta.mu = 0
beta.sigma = 0.8
beta.unif.min = -2
beta.unif.max = 2
int.mu = 0
int.sd = 0.1
beta.selection.cutoff = 0.05

#HIV+ and HIV- subjects
prob.hiv.pos = 0.2
age.mean.hiv.neg = 0.5
age.mean.hiv.pos = -0.5
age.sd = 0.8

age2.mean = 45
age2.sd = 13

### Pu and Ps
a.u = 50
a.s = 100

#Concentration params drawn from lognormal
pu.logmu = log(20)
p.logsd = 0.03
#Diff in means of lognormal a function of covariate with true intercepts and slopes below
ps.logmu.diff.beta = c(0.5, 1)
ps.logmu.diff.sd = 0.1 #Error term for relationship between shift in concentration param and covariate

### Protection
beta.p = c(-2, 2)

#Figure output
out.dir = file.path("/work",
                        Sys.getenv("USER"),
                        "COMPASScovariate-sim",
                        "output"
)
dir.create(out.dir,
           showWarnings = FALSE,
           recursive = TRUE
)

#Function to adjust concentration parameters depending on the number of responding cell subsets
conc.adjust <- function(ng, k, alpha, p) {
  #Calculate prop of subsets responding (gamma = 1)
  prop = ng/k
  if(prop < p) {
    #If the proportion is less than p, then reduce the concentration parameter as a linear function of that proportion within the range of ng < p
    return(alpha * prop/p)
  } else {
    #Otherwise return alpha unchanged
    return(alpha)
  }
}
```

### Generate combinations of simulation parameters

```{r}
#Get number of threads from SLURM scheduler as # of CPUs allocated
# threads = as.integer(Sys.getenv("SLURM_CPUS_PER_TASK"))
threads = 12

#Make a single df of all combinations of parameters
sim.pars <- 
  expand.grid(I = c(50),#, 100, 200),   #Number of subjects
              K = c(16),        #Number of unique cell subsets
              p = c(0),         #Number of covariates IN ADDITION TO AGE | HIV status
              # prob.hiv.pos = c(0.05, 0.1, 0.5),    #Proportion with binary indicator == 1
              beta.unif.min = c(-2, -3, -4),         #Lower bound on beta1 distribution
              beta.unif.max = c(0, 0, 0)          #Upper bound on beta1 distribution
              
  ) %>%
  
  #Remove unwanted combinations
  filter(beta.unif.min <= beta.unif.max &
         beta.unif.max - beta.unif.min <= 4
  ) %>%
  filter(!duplicated(.))

#Repeat the df to create replicates for each combination -- repeat 'sim.rep' times
sim.pars <- do.call('rbind', lapply(1:sim.rep, FUN = function(simrep) {
  data.frame(
    sim_rep = simrep,
    sim.pars
  )
  })
)
```

### Run Simulation

```{r}
cl <- makeCluster(threads)
registerDoParallel(cl)

sim.out.list <- foreach(s = 1:nrow(sim.pars), .packages = c("COMPASS", "dplyr", "tidyr", "MCMCpack")) %dopar% {

  ##################
  # Fixed Parameters
  ##################
  I <- sim.pars[s,]$I ## sample size i.e., #subjects; I = 30
  K <- sim.pars[s,]$K ## number of cell categories: K = 16
  p <- sim.pars[s,]$p + 1 # number of covariates # larger p
  prob.hiv.pos <- sim.pars[s,]$prob.hiv.pos
  beta.unif.min <- sim.pars[s,]$beta.unif.min
  beta.unif.max <- sim.pars[s,]$beta.unif.max
  sim_rep <- sim.pars[s,]$sim_rep
  K1 = K-1 ## number of cell categories minus one (last one is baseline)
  
  #########################
  ### True Regression Coefs
  #########################
  #Intercepts
  int.true = rnorm(n = K1,
                   mean = int.mu,
                   sd = int.sd
  )
  
  #Covariate betas
  # beta.true = matrix(rnorm(n = K1*(p),
  #                          mean = beta.mu,
  #                          sd = beta.sigma),
  #                    nrow = K1
  # )
  beta.true = matrix(runif(n = K1*(p),
                           min = beta.unif.min,
                           max = beta.unif.max
  ),
  nrow = K1
  )
  #Convert all but 6 betas to 0
  # beta.true[sample(1:length(beta.true),size = K1 - 6)] = 0
  
  #Variable selection -- if |true beta| < 0.5, then true beta = 0
  beta.true.selection = apply(beta.true, MARGIN = c(1,2), FUN = function(x) ifelse(abs(x) < beta.selection.cutoff, 0, x))
  
  #Bind intercept betas to other betas
  beta.true.final <- cbind(int.true, beta.true.selection)
  
  ##################
  ###Make covariates
  ##################
  #Add int and HIV status
  covar <- data.frame(
    #First column is 1s for intercept
    int = rep(1, I*1),
    #HIV status
    # hiv = sample(c(1, 0), size = I, replace = T, prob = c(prob.hiv.pos, 1-prob.hiv.pos)),
    #Age
    # age = rep(NA, I)
    age2 = rnorm(n = I,
                 mean = age2.mean,
                 sd = age2.sd
      ) %>%
      scale(center = TRUE, scale = TRUE)
  )
  
  #Add remainder of covariates if any
  if (p > 1) {
    covar <- cbind(covar,
                   matrix(rnorm(n = I),
                          ncol = p-1,
                          nrow = I,
                          dimnames = list(NULL, paste0("V", 2:(p)))
                   )
    )
  }
  
  #Add age | HIV
  # covar <- covar %>%
  #   mutate(age = replace(age, hiv == 0, rnorm(n = sum(hiv == 0),
  #                                             mean = age.mean.hiv.neg,
  #                                             sd = age.sd
  #   )
  #   ),
  #   age = replace(age, hiv == 1, rnorm(n = sum(hiv == 1),
  #                                      mean = age.mean.hiv.pos,
  #                                      sd = age.sd
  #   )
  #   )
  #   )
  #Plot age | HIV
  # covar %>%
  #   ggplot(aes(x = age,
  #              fill = as.factor(hiv))
  #   )+
  #   geom_density(alpha = 0.7)+
  #   theme_bw()
  
  #Matrix matrix version without HIV status
  X = covar %>%
    # dplyr::select(-age2) %>%
    as.matrix()
  
  #####################
  ### True Gamma Matrix
  #####################
  
  #Generate omega_ik -- probability that gamma_ik = 1
  
  #  1. p covariates from subject i x betas for p coefficients for cluster k gives linear predictor for cluster k in subject i
  #  2. Inverse-logit transform linear predictor to obtain prob that gamma = 1 (true diff between stim and unstim) for cluster k in subject i
  
  #Empty matrix to hold omega
  omega <- matrix(NA,
                  nrow = I,
                  ncol = K1
  )
  
  #Empty matrix to hold gamma
  gamma.m <- matrix(NA,
                    nrow = I,
                    ncol = K1
  )
  
  for (i in 1:I) {
    for (k in 1:K1) {
      #Generate linear predictor and prob via inverse link
      omega[i,k] <- binomial()$linkinv(X[i, ] %*% beta.true.final[k,]) #p covariates from ith subject x betas for p coefficients for kth cluster gives linear predictor
  
      #Generate gamma as Bernoulli RV with probs from omega
      gamma.m[i,k] <- rbinom(n = 1,
                             size = 1,
                             prob = omega[i,k]
      )
    }
  }
  
  
  
  
  ######################
  ### Counts
  ######################
  
  ### Empty counts matrices
  #Empty matrices for stim and unstim counts. **Note:** Create K columns with 0s, not K - 1
  n_u <- matrix(0,
                nrow = I,
                ncol = K
  )
  
  n_s <- matrix(0,
                nrow = I,
                ncol = K
  )
  
  ### Total counts
  ## Unstim
  Nu = ceiling(
    rnorm(mean = e.total.mu,
          sd = e.total.sd,
          n = I)
  )
  ## Stim
  Ns = ceiling(
    rnorm(mean = e.total.mu,
          sd = e.total.sd,
          n = I)
  )
  
  
  #############################
  ### Generate Counts
  #############################
  
  #########
  ### Probs
  #Empty matrices for probs
  p_u = matrix(NA,
               nrow = I,
               ncol = K
  )
  p_s = p_u
  
  #################
  ### Concentration
  
  # Fixed concentration vector for unstim and stim for all samples
  # Unstim: K-1 subsets have value a.u, Kth subset has value 10 x a.u to generate large counts
  # Stim: K-1 subsets have value a.s, Kth subset has value 10 x a.s to generate large counts (but not used in practice)
  
  #Unstim
  alpha_u = matrix(c(rep(a.u, K1), a.u*10),
                   nrow = I,
                   ncol = K,
                   byrow = TRUE
  )
  
  #Stim
  alpha_s = matrix(c(rep(a.s, K1), a.u*10),
                   nrow = I,
                   ncol = K,
                   byrow = TRUE
  )
  
  
  #############################
  ### Generate Probs and Counts
  
  # For each sample:
  #   Create an unstim prob vector of length K drawn from Dirichelet using the unstim concentration vector
  #   Create a stim prob vector of length K by copying the unstim
  #   For subsets where gamma == 1 (and for the Kth subset), draw new probs from Dirichlet using stim concentration vector, then normalize by the total prob in the unstim vector of these entires
  #     IMPORTANT:  Include the Kth subset in new Dirichlet and normalization so that the increased prob of response among k-1 subsets can be "taken" from the Kth subset
  
  for (i in 1:I) {
    # Draw prob vector from Dirichlet for unstim
    p_u[i,] <- rdirichlet(n = 1, alpha = alpha_u[i,])
  
    # Copy prob vector for unstim and assign to stim
    p_s[i,] <- p_u[i,]
  
    # For subsets where gamma == 1, draw new probabilities from Dirichlet using stim concentrations, and normalize (see note above)
    p_s[i, c(which(gamma.m[i,] == 1), K)] <- 
      #Draw new probability vector for just subsets where gamma = 1
      rdirichlet(n = 1,
                 #Concentration vector is alpha_s for subsets where gamma = 1 and for Kth subset
                 alpha = c(#Adjust the alpha_s for subsets where gamma = 1
                           conc.adjust(ng = sum(gamma.m[i,] == 1),
                                       k = K,
                                       p = 0.25,
                                       alpha = alpha_s[i, which(gamma.m[i,] == 1)]
                          ),
                          #Alpha for Kth subsets (not adjusted)
                          alpha_s[i, K]
                 )
                 
          #Normalize the new probability vector by the total probabilty for subsets where gamma = 1 and the kth subset
          #This ensures new p_s is still a valid probability vector
      ) * sum(p_u[i, c(which(gamma.m[i,] == 1), K)])
  
    # Draw counts for untim from Multinomial using unstim prob vector
    n_u[i,] <- rmultinom(n = 1,
                         size = Nu[i],
                         prob = p_u[i,]
    )
  
    # Draw counts for stim from Multinomial using stim prob vector
    n_s[i,] <- rmultinom(n = 1,
                         size = Ns[i],
                         prob = p_s[i,]
    )
  }

  
  ## Add colnames
  colnames(n_s) = 1:K
  colnames(n_u) = 1:K
  
  ## Add rownames
  rownames(n_s) = 1:I
  rownames(n_u) = 1:I
  
  ##################################
  ## Generate Polyfunctional Scores
  ##################################
  
  #Function
  PolyfunctionalityScore.default <- function(categories, gamma) {
    degree <- categories[, "Counts"]
    n <- ncol(categories) - 1
    pfs <-
      apply(gamma, 1, function(row) {
        ## (2 / (n+1)) is a factor that normalized the score between 0 and 1
        sum(row * degree / choose(n, degree)) / n * (2 / (n + 1))
      })
    
    return(pfs)
  }
  
  #Calculate the categories (copied from simpleCOMPASS.R)
  marker_names <- unique(
    unlist( strsplit( gsub("!", "", colnames(n_s)), "&", fixed=TRUE ) )
  )
  n_markers <- length(marker_names)
  cats <- as.data.frame( matrix(0, nrow=ncol(n_s), ncol=n_markers) )
  rownames(cats) <- colnames(n_s)
  colnames(cats) = marker_names
  for (i in seq_along(cats)) {
    #cats[, i] <- as.integer(grepl( paste0( colnames(cats)[i], "+" ), rownames(cats), fixed=TRUE ))
    cats[,i] <-
      as.integer(!grepl(paste0("!",colnames(cats)[i],"(&|$)+"),rownames(cats),fixed =
                          FALSE))
  }
  cats$Counts <- apply(cats, 1, sum)
  cats <- as.matrix(cats)
  
  #Modify gamma.m to add a column for the null subset (all 0s because none are stimulated)
  gamma.m.mod <- cbind(gamma.m, rep(0, nrow(gamma.m)))
  
  #Calulate PFS
  pfs <-
    PolyfunctionalityScore.default(categories = cats,
                                 gamma = gamma.m.mod
    )
  
  #Add intercept
  pfs <-
    data.frame(int = rep(1, I),
               pfs = pfs
    ) %>%
    as.matrix()
  
  ####################################
  ## Generate Outcome (HIV protection)
  ####################################
  #Generate linear predictor and prob via inverse link
  protect.prob <- binomial()$linkinv(pfs %*% beta.p) #p covariates from ith subject x betas for p coefficients for kth cluster gives linear predictor
  
  #Generate gamma as Bernoulli RV with probs from omega
  protect.true <- rep(NA, length(protect.prob))
  for(i in 1:length(protect.prob)) {
    protect.true[i] <-
      rbinom(n = 1,
             size = 1,
             prob = protect.prob[i]
      )
  }
  
  ##
  covar <-
    data.frame(covar,
               pfs.true = pfs,
               protect.prob.true = protect.prob,
               protect.true = protect.true
    )
  
  ########################
  ### Run COMPASScovariate
  ########################
  
  ## Make metadata for COMPASS
  metadat <- 
    covar %>%
    dplyr::select(-int) %>%
    mutate(sample.id = rownames(.))
  
  #Create a version of X without an intercept column
  X.mod = as.matrix(X[,-1])  #Use as.matrix to keep matrix format in case ncol(X.mod) = 1
  
  #COMPASScovariate
  .fit = .COMPASS.covariate(n_s = n_s, 
                            n_u = n_u,
                            X = X.mod,
                            iterations = iter, 
                            replication = rep
  )
  
  #COMPASS
  .compass_fit = SimpleCOMPASS(n_s = n_s,
                               n_u = n_u,
                               meta = metadat,
                               individual_id = "sample.id",
                               iterations = iter,
                               replications = rep
  )
  
  ###Coerce fit into output used in simpleCOMPASS.R
  fit <- list(
    fit=.fit,
    data=list(
      n_s=n_s,
      n_u=n_u,
      counts_s=rowSums(n_s),
      counts_u=rowSums(n_u),
      categories=.fit$categories,
      meta=covar
      # individual_id=individual_id
    )
  )
  class(fit) <- c("COMPASSResult")
  
  compass.fit <- list(
    fit = .compass_fit,
    data=list(
      n_s=n_s,
      n_u=n_u,
      counts_s=rowSums(n_s),
      counts_u=rowSums(n_u),
      categories=.compass_fit$categories,
      meta=covar
      # individual_id=individual_id
    )
  )
  class(compass.fit) <- c("COMPASSResult")
  
  ###################
  ### Collect Results
  ###################
  results.list <- list(fit,
                       compass.fit,
                       gamma.m,
                       omega,
                       beta.true.final,
                       beta.p,
                       I,
                       K,
                       K1,
                       p,
                       sim_rep,
                       # prob.hiv.pos,
                       beta.unif.min,
                       beta.unif.max
  )
  
  names(results.list) <- c('sim.fit',
                           'compass.fit',
                           'gamma.m',
                           'omega',
                           'beta.true.final',
                           'beta.p',
                           'I',
                           'K',
                           'K1',
                           'p',
                           'sim_rep',
                           # 'prob.hiv.pos',
                           'beta.unif.min',
                           'beta.unif.max'
                          )
  
  return(results.list)
}

stopCluster(cl)
```

**Notes on including cats**

## Examine

### Acceptance Rate

Check acceptance rate for sampler for COMPASS and COMPASScovariate (want 20% - 30%). If too low, need to tune the algorithm. 
```{r}
lapply(sim.out.list, FUN = function(fit) {
  data.frame(
    N.subj = fit$I,
    # per.covar = fit$prob.hiv.pos,
    beta.unif.min = fit$beta.unif.min,
    beta.unif.max = fit$beta.unif.max,
    metric = c("Min", "Q1", "Median", "Mean", "Q3", "Max"),
    COMPASScovar = as.numeric(summary(fit$sim.fit$fit$A_gamma)),
    COMPASS = as.numeric(summary(fit$compass.fit$fit$fit$A_gamma))
  )
}) %>%
  do.call('rbind', .) %>%
  group_by(N.subj, beta.unif.min, beta.unif.max, metric) %>%
  summarize(Mean.COMPASScov = mean(COMPASScovar),
            Mean.COMPASS = mean(COMPASS)
  ) %>%
  filter(metric %in% c("Mean"))
```



### Mean Gamma vs Omega

#### Summaries

```{r}
do.call('rbind',
  lapply(sim.out.list, FUN = function(sim) {
  
    #Omega - mean_gamma
    omega.diff.covar <- sim$omega - sim$sim.fit$fit$mean_gamma[,1:sim$K1]
    omega.diff.compass <- sim$omega - sim$compass.fit$fit$fit$mean_gamma[,1:sim$K1]
    colnames(omega.diff.compass) <- NULL
    
    #Create datasets
    omega.diff.covar.df <- data.frame(age2 = as.factor(sim$sim.fit$data$meta$age2),
                                            omega.diff.covar
    )
    omega.diff.compass.df <- data.frame(age2 = as.factor(sim$sim.fit$data$meta$age2),
                                              omega.diff.compass
    )
    
    #Combine datasets in long format
    omega.diff.df <-
      bind_rows(
        omega.diff.covar.df %>%
          mutate(model = "COMPASScovariate"),
        omega.diff.compass.df %>%
          mutate(model = "COMPASS")
      )
    
    #Rename columns
    colnames(omega.diff.df) <- gsub("^X", "cluster", colnames(omega.diff.df))
    
    #Calculate per gamma correct
    omega.diff.df %>%
      
      #Group by model
      group_by(model) %>%
      
      #Loop over cell subsets and find sum of squared error (omega - mean_gamma)^2
      summarize_at(vars(cluster1:cluster15),
                   ~ sum(abs(.x))
      ) %>%
      ungroup() %>%
      
      #Find total SSE across all cell subsets
      mutate(SAE.total = rowSums(across(starts_with("cluster")))
      ) %>%
      
      #Find COMPASScov - COMPASS in SSE
      bind_rows(summarise(.,
                      across(where(is.numeric), ~ .x[model == "COMPASScovariate"] - .x[model == "COMPASS"]),
                      across(where(is.character), ~ "COMPASScovariate - COMPASS")
                )
      ) %>%
      
      #Find proportion reduction in SSE
      bind_rows(summarise(.,
                      across(where(is.numeric), ~ .x[model == "COMPASScovariate"] / .x[model == "COMPASS"]),
                      across(where(is.character), ~ "COMPASScovariate / COMPASS")
                )
      ) %>%
      
      relocate(SAE.total, .after = model) %>%

      mutate(#per.covar = sim$prob.hiv.pos,
             N.subj = sim$I,
             beta.unif.min = sim$beta.unif.min,
             beta.unif.max = sim$beta.unif.max
      ) %>%
      # relocate(per.covar, .before = SSE.total) %>%
      relocate(beta.unif.min, .before = SAE.total) %>%
      relocate(beta.unif.max, .after = beta.unif.min)


    
    })
  ) %>%
  group_by(N.subj, beta.unif.min, beta.unif.max, model) %>%
  summarize(N.rep = n(),
            mean.SAE.total = mean(SAE.total)
  )
```


### Mean Gamma vs True Gamma

#### Summaries


Percent gamma correct
```{r}
do.call('rbind',
  lapply(sim.out.list, FUN = function(sim) {
  
    #Gamma - mean_gamma
    true.gamma.diff.covar <- sim$gamma.m - sim$sim.fit$fit$mean_gamma[,1:sim$K1]
    true.gamma.diff.compass <- sim$gamma.m - sim$compass.fit$fit$fit$mean_gamma[,1:sim$K1]
    colnames(true.gamma.diff.compass) <- NULL
    
    #Create datasets
    true.gamma.diff.covar.df <- data.frame(true.gamma.diff.covar)
    true.gamma.diff.compass.df <- data.frame(true.gamma.diff.compass)
    
    #Combine datasets in long format
    true.gamma.diff.df <-
      bind_rows(
        true.gamma.diff.covar.df %>%
          mutate(model = "COMPASScovariate"),
        true.gamma.diff.compass.df %>%
          mutate(model = "COMPASS")
      )
    
    #Rename columns
    colnames(true.gamma.diff.df) <- gsub("^X", "cluster", colnames(true.gamma.diff.df))
    
    #Calculate per gamma correct
    true.gamma.diff.df %>%
      
      #Group by model
      group_by(model) %>%
      
      #Loop over cell subsets and count number of times the abs. value of gamma.m - mean_gamma > 0.5
      #This corresponds to counting incorrect estimates of gamma with a cutoff value of 0.5
      summarize_at(vars(cluster1:cluster15),
                   ~ sum(abs(.x) > 0.5)
      ) %>%
      ungroup() %>%
      
      #Find N.gamma = number of total gammas estimates (K-1) * I
      #and N.wrong = the sum of the counts above
      mutate(N.gamma = length(across(where(is.numeric))) * sim$I,
             N.wrong = rowSums(across(starts_with("cluster")))
      ) %>%
      
      #Find percent wrong
      mutate(Per.wrong = N.wrong / N.gamma * 100,
             Per.correct = 100 - Per.wrong
      ) %>%
      relocate(N.gamma, .before = cluster1) %>%
      relocate(N.wrong, .after = N.gamma) %>%
      relocate(Per.wrong, .after = N.wrong) %>%
      relocate(Per.correct, .after = Per.wrong) %>%
      
      mutate(#per.covar = sim$prob.hiv.pos,
             N.subj = sim$I,
             beta.unif.min = sim$beta.unif.min,
             beta.unif.max = sim$beta.unif.max
      ) %>%
      # relocate(per.covar, .before = N.gamma) %>%
      relocate(beta.unif.min, .before = N.gamma) %>%
      relocate(beta.unif.max, .after = beta.unif.min)
  
  
    
  })
) %>%
  group_by(N.subj, beta.unif.min, beta.unif.max, model) %>%
  summarize(N.rep = n(),
            N.gamma = mean(N.gamma),
            N.wrong = mean(N.wrong),
            Per.wrong = mean(Per.wrong),
            Per.correct = mean(Per.correct)
  )
```


### Variance of Gamma Posteriors

Is the precision of the estimates lower for COMPASS without a covariate?

```{r}
do.call('rbind', 
  lapply(sim.out.list, FUN = function(sim) {
    data.frame(
      tot.var.compass.cov = sum(apply(sim$sim.fit$fit$gamma, MARGIN = c(1,2), var)),
      tot.var.compass = sum(apply(sim$compass.fit$fit$fit$gamma, MARGIN = c(1,2), var))
    ) %>%
    mutate(
      C.minus.CC = tot.var.compass - tot.var.compass.cov,
      C.prop.CC = tot.var.compass.cov / tot.var.compass,
      sim.rep = sim$sim_rep,
      N.subjects = sim$I,
      # Prob.covar = sim$prob.hiv.pos,
      Beta.unif.min = sim$beta.unif.min,
      Beta.unif.max = sim$beta.unif.max
  )
  })
) %>%
  group_by(N.subjects, Beta.unif.min, Beta.unif.max) %>%
  summarize(N.rep = n(),
            Mean.C.minus.CC = mean(C.minus.CC),
            Mean.C.prop.CC = mean(C.prop.CC)
  )
```


### Beta


```{r}
do.call('rbind', 
  lapply(sim.out.list, FUN = function(sim) {
  
    #Calculate mean of posterior
    data.frame(
      param = "beta1",
      true.mean = sim$beta.true.final[,2],
      post.mean = apply(sim$sim.fit$fit$beta, MARGIN = c(1,2), FUN = function(x) mean(x))[,2],
      post.LCL.95 = apply(sim$sim.fit$fit$beta, MARGIN = c(1,2), FUN = function(x) quantile(x, 0.025))[,2],
      post.UCL.95 = apply(sim$sim.fit$fit$beta, MARGIN = c(1,2), FUN = function(x) quantile(x, 0.975))[,2]
      ) %>%
      bind_rows(.,
                data.frame(
                  param = "beta0",
                  true.mean = sim$beta.true.final[,1],
                  post.mean = apply(sim$sim.fit$fit$beta, MARGIN = c(1,2), FUN = function(x) mean(x))[,1],
                  post.LCL.95 = apply(sim$sim.fit$fit$beta, MARGIN = c(1,2), FUN = function(x) quantile(x, 0.025))[,1],
                  post.UCL.95 = apply(sim$sim.fit$fit$beta, MARGIN = c(1,2), FUN = function(x) quantile(x, 0.975))[,1]
                )
      ) %>%
      mutate(within = true.mean >= post.LCL.95 & true.mean <= post.UCL.95,
             under.est = true.mean > post.UCL.95,
             over.est = true.mean < post.LCL.95
      ) %>%
      
      group_by(param) %>%
      
      summarize(Per.beta.within = sum(within) / n() * 100,
                Per.beta.under = sum(under.est) / n() * 100,
                Per.beta.over = sum(over.est) / n() * 100
      ) %>%
      
      mutate(#per.covar = sim$prob.hiv.pos,
             N.subj = sim$I,
             beta.unif.min = sim$beta.unif.min,
             beta.unif.max = sim$beta.unif.max
      ) %>%
      # relocate(per.covar, .before = param) %>%
      relocate(beta.unif.min, .before = param) %>%
      relocate(beta.unif.max, .after = beta.unif.min)
      
  })
) %>%
  group_by(N.subj, beta.unif.min, beta.unif.max, param) %>%
  summarize(N.rep = n(),
            Mean.per.within = mean(Per.beta.within),
            Mean.per.under = mean(Per.beta.under),
            Mean.per.over = mean(Per.beta.over)
  )
```

