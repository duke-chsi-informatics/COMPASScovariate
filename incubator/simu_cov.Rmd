---
title: "COMPASScovariate Test Simulations"
author: "Tyler Schappe"
date: "2022-11-22"
output: 
  html_document:
    code_folding: hide
  html_notebook:
    code_folding: show
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Source for now but will want to call using COMPASS.R eventually
source("~/project_repos/COMPASScovariate/R/COMPASS-covariate.R")
source("~/project_repos/COMPASScovariate/R/updatebeta.R")
source("~/project_repos/COMPASScovariate/R/utils.R")

library(foreach); library(doParallel); library(HMP); library(MultiRNG); library(MCMCpack); library(devtools); library(ggplot2)

# install.packages("MultiRNG")
```

## Notes

Alternative sim method (prioritize this one):

1. Generate X cov
2. Specify true betas (non-zero simulated from Normal)
3. Run logistic regression to compute p-hat
4. Using p-hat, generate Gamma matrix via rbinom()
5. Generate pu, which is props for unstim
6. Generate counts via multinomial
7. Conditioned on gamma and pu, generate ps
8. Generate ns (counts) from ps (prob vector)


Using real data (COMPASS vignette):

1. Run COMPASS on real data
2. Get Gamma matrix
3. Get ns and nu (counts)
4. Generate X 
5. Run logistic regression (glmnet with lasso) between gamma and X. Estimated beta will be considered "true" beta. 

**Note on iterations:**

Keep only the last X iterations and use the first ones to warm up chains.

**12/13/22 To Do:**

- Create density plot of posterior for betas that have a large difference between estimated and true.
- Reduce to 2 markers = 4 cell subsets
- Modify covariates (intercept) to allow ~ 50% of gammas to be 1

### Additional Complexities

  1. Smaller difference in concentration parameters between stim and unstim
  2. Different number of total events and event subsets for stim and unstim samples


## Method 1

**Approach**

  1. Matrix of true betas for each k-1 subset for each covariate p ~ Normal(beta.mu, beta.sigma)
    1. Model selection where any beta where |beta| < beta.selection.cutoff -> 0
  2. Vector of true intercepts for each k-1 subset ~ Normal(int.mu, int.sd)
    1. int.mu is negative to keep probability that gamma = 1 small to give a small number of stimulated subsets for simplicity
  3. Matrix X (I x p) of covariates with first column of 1's for intercept
  4. Omega (I x K1): For each sample, for each subset, generate linear predictor as linear combination of sample's covariate values X[i,] and subset-specific true betas beta.true.final[k,]
  5. gamma.m (I x K1): For each sample, for each subset, map linear predictor to probability using inverse-logit function
  6. Generate unstimulated and stimulated counts from a Dirichlet-Multinomial distribution where the concentration parameters are drawn from a lognormal distribution with same logmean and logsd values. 
    1. For stimulated cluster-samples (where gamma = 1), shift the logmean by ps.logmu.diff to increase the concentration parameter by exp(ps.logmu.diff).
    2. For each sample, number of stimulated and unstimulated counts ~ Normal(e.total.mu, e.total.sd). This is the total number of stim and unstim counts for each sample.
    3. For both stimluated and unstimulated, use Dirichlet-Multinomial to draw subset of stimulated and unstimulated counts ~ Normal(e.subset.mu, e.subset.sd). This is the number of stim and unstim counts for the k-1 subsets. **Note:** The values of the concentration parameter can be interpreted as prior pseudo-counts and thus determine the variance among categories. 
    4. Find the counts in the kth subset (corresponding to all negative cell subset) as the total number of counts minus the sum of the first k - 1 subsets
      + This implies that the difference between stim and unstim for the kth subset is not a function of the betas
  

### Set Fixed Parameters

```{r}
set.seed(111)

I <- 200 ## sample size i.e., #subjects
K <- 4 ## number of cell categories
K1 = K-1 ## number of cell categories minus one (last one is baseline)
p <- 5 # number of covariates # larger p
e.total.mu = 10000 #Mean total number of events per subject
e.total.sd = 10   #SD of total number of events per subject
e.subset.mu = 3000 #Mean number of events per subject that are not in the Kth category (all negative set)
e.subset.sd = 5   #SD of number of events per subject that are not in the Kth category (all negative set)

#Betas
beta.mu = 0
beta.sigma = 0.3
int.mu = 0
int.sd = 0.1
beta.selection.cutoff = 0.1

### Pu and Ps
#Concentration params drawn from lognormal
pu.logmu = log(20)
p.logsd = 0.01
ps.logmu.diff = log(5)

### COMPASScovariate
iter = 40000
rep = 2
```

### True Regression Coefs

Set true betas
```{r}
#Intercepts
int.true = rnorm(n = K1,
                 mean = int.mu,
                 sd = int.sd
                 )

#Covariate betas
beta.true = matrix(rnorm(n = K1*(p),
                         mean = beta.mu, 
                         sd = beta.sigma), 
                   nrow = K1
                  )
```

Variable selection -- if |true beta| < 0.5, then true beta = 0
```{r}
beta.true.selection = apply(beta.true, MARGIN = c(1,2), FUN = function(x) ifelse(abs(x) < beta.selection.cutoff, 0, x))
```

Bind intercept betas to other betas
```{r}
beta.true.final <- cbind(int.true, beta.true.selection)
```

Make covariates
```{r}
X <- cbind(
      #First column is 1s for intercept
      rep(1, I*1),
      #Remainder are for covariates
      matrix(rnorm(n = I*p), 
            ncol = p, 
            nrow = I
      )
)
```

### True Gamma Matrix

Generate omega_ik -- probability that gamma_ik = 1

  1. p covariates from subject i x betas for p coefficients for cluster k gives linear predictor for cluster k in subject i
  2. Inverse-logit transform linear predictor to obtain prob that gamma = 1 (true diff between stim and unstim) for cluster k in subject i
  
```{r}
#Empty matrix to hold omega
omega <- matrix(NA,
                nrow = I,
                ncol = K1
                )

#Empty matrix to hold gamma
gamma.m <- matrix(NA,
                     nrow = I,
                     ncol = K1
                     )

for (i in 1:I) {
  for (k in 1:K1) {
    #Generate linear predictor and prob via inverse link
    omega[i,k] <- binomial()$linkinv(X[i, ] %*% beta.true.final[k,]) #p covariates from ith subject x betas for p coefficients for kth cluster gives linear predictor
    
    #Generate gamma as Bernoulli RV with probs from omega
    gamma.m[i,k] <- rbinom(n = 1,
                           size = 1,
                           prob = omega[i,k]
                           )
  }
}
```


### True Counts

Empty matrices for stim and unstim counts. **Note:** Create K columns with 0s, not K - 1
```{r}
n_u <- matrix(0,
              nrow = I,
              ncol = K)

n_s <- matrix(0,
              nrow = I,
              ncol = K)
```


#### Unstimulated

Generate unstimulated count values for each subject
```{r}
#Generate total counts
Nu = ceiling(
      rnorm(mean = e.total.mu, 
            sd = e.total.sd,
            n = I)
)

Nu.subset = ceiling(
              rnorm(mean = e.subset.mu, 
                    sd = e.subset.sd,
                    n = I
              ) 
)
```

Loop over each sample and do two things:

  1. Create a vector of length K - 1 (not K) of concentration parameters drawn from a lognormal distribution with an unstimulated-specific log-mean.
  2. Using the vector of concentration parameters, draw a vector of counts of length K - 1 (not K) from a Dirichlet-Multinomial distribution and fill the first K - 1 columns of the ith row of n_u

**Note:** Leave the last column as 0 for all negative cell subset

```{r}
concentration_u = matrix(NA,
                         nrow = I,
                         ncol = K1
                  )

for (i in 1:I) {
  concentration_u[i,] = rlnorm(n = K1, 
                 meanlog = (pu.logmu), 
                 sdlog = p.logsd
  )
  #HMP version
  # n_u[i,1:K1] = t(matrix(HMP::Dirichlet.multinomial(Nrs = Nu.subset[i], shape = concentration_u[i,])))
  
  #MultiRNG version
  n_u[i,1:K1] = MultiRNG::draw.dirichlet.multinomial(N = Nu.subset[i],                    #Total counts
                                                         no.row = 1,                      #Sample size
                                                         d = length(concentration_u[i,]), #Number of categories (same as length of concentration vector)
                                                         alpha = concentration_u[i,],     #Concentration vector
                                                         beta = 1                         #Common shape parameter
                                                              )
}
```

Check rowsums
```{r}
identical(rowSums(n_u), Nu.subset)
```

Make the kth cluster be the remaining events for the sample
```{r}
n_u[,K] = Nu - rowSums(n_u)
```

Check rowsums
```{r}
identical(rowSums(n_u), Nu)
```

#### Stimulated

Generate stimulated count values for each subject
```{r}
#Generate total counts
Ns = ceiling(
      rnorm(mean = e.total.mu, 
            sd = e.total.sd,
            n = I)
)

Ns.subset = ceiling(
              rnorm(mean = e.subset.mu, 
                    sd = e.subset.sd,
                    n = I
              ) 
)
```

Loop over each sample and do two things:

  1. Create a vector of length K - 1 (not K) of concentration parameters drawn from a lognormal distribution with an unstimulated-specific log-mean.
  2. Using the vector of concentration parameters, draw a vector of counts of length K - 1 (not K) from a Dirichlet-Multinomial distribution and fill the first K - 1 columns of the ith row of n_u

**Notes:** 

  - Leave the last column as 0
  - Concentration is shifted by ps.logmu.diff if gamma = 1

```{r}
concentration_s = matrix(NA,
                         nrow = I,
                         ncol = K1
                  )

for (i in 1:I) {
  concentration_s[i, ] = rlnorm(n = K1, 
                 meanlog = (pu.logmu + ps.logmu.diff * gamma.m[i, ]), 
                 sdlog = p.logsd
  )
  #HMP version
  # n_s[i,1:K1] = t(matrix(HMP::Dirichlet.multinomial(Nrs = Ns.subset[i], shape = concentration_s[i,])))
  
  #MultiRNG version
  n_s[i,1:K1] = MultiRNG::draw.dirichlet.multinomial(N = Ns.subset[i],                #Total counts
                                                     no.row = 1,                      #Sample size
                                                     d = length(concentration_s[i,]), #Number of categories (same as length of concentration vector)
                                                     alpha = concentration_s[i,],     #Concentration vector
                                                     beta = 1                         #Common shape parameter
                                                              )
}

```

Check rowsums
```{r}
identical(rowSums(n_s), Ns.subset)
```

Make the kth cluster be the remaining events for the sample
```{r}
n_s[,K] = Ns - rowSums(n_s)
```

Check rowsums
```{r}
identical(rowSums(n_s), Ns)
```

### Run COMPASScovariate

Create a version of X without an intercept column
```{r}
X.mod = X[,-1]
```

```{r message=FALSE, warning=FALSE}
sim1.fit = .COMPASS.covariate(n_s = n_s, 
                             n_u = n_u,
                             X = X.mod,
                             iterations = iter, 
                             replication = rep)
```

### Compare Estimates to True

Create a custom function for calculating the mode
```{r}
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
```

#### Gamma

```{r}
sim1.fit$mean_gamma[1:2,]
```

```{r}
apply(sim1.fit$gamma, MARGIN = c(1,2), FUN = function(x) getmode(x))[1:2,]
```


```{r}
gamma.m[1:2,]
```

```{r}
n_u[1:2,]
n_s[1:2,]
```


#### Betas

```{r}
apply(sim1.fit$beta, MARGIN = c(1,2), FUN = function(x) median(x))
```

```{r}
beta.true.final
```

## Method 2

**Approach**: An iteration of Method 1 above except allow the total number of counts in the k-1 category to increase depending on the number of gammas = 1 for that sample.

**Note:** Re-use the regression coefs and gamma matrix from method 1 above.

### True Counts

Empty matrices for stim and unstim counts. **Note:** Create K columns with 0s, not K - 1
```{r}
n_u <- matrix(0,
              nrow = I,
              ncol = K)

n_s <- matrix(0,
              nrow = I,
              ncol = K)
```


#### Unstimulated

Generate unstimulated count values for each subject
```{r}
#Generate total counts
Nu = ceiling(
      rnorm(mean = e.total.mu, 
            sd = e.total.sd,
            n = I)
)

Nu.subset = ceiling(
              rnorm(mean = e.subset.mu, 
                    sd = e.subset.sd,
                    n = I
              ) 
)
```

Loop over each sample and do two things:

  1. Create a vector of length K - 1 (not K) of concentration parameters drawn from a lognormal distribution with an unstimulated-specific log-mean.
  2. Using the vector of concentration parameters, draw a vector of counts of length K - 1 (not K) from a Dirichlet-Multinomial distribution and fill the first K - 1 columns of the ith row of n_u

**Notes:** 

- Leave the last column as 0 for all negative cell subset
- Use Nu instead of Nu.subset because now Kth category included in DM-generated data


```{r}
concentration_u = matrix(NA,
                         nrow = I,
                         ncol = K
                  )

for (i in 1:I) {
  concentration_u[i,1:K1] = rlnorm(n = K1, 
                 meanlog = (pu.logmu), 
                 sdlog = p.logsd
  )
  
  concentration_u[i,K] = rlnorm(n = 1,
                                meanlog = pu.logmu*3,
                                sdlog = p.logsd
  )
  #HMP version
  n_u[i,1:K] = t(matrix(HMP::Dirichlet.multinomial(Nrs = Nu[i], shape = concentration_u[i,])))
  
  # #MultiRNG version
  # n_u[i,1:K] = MultiRNG::draw.dirichlet.multinomial(N = Nu[i],                    #Total counts
  #                                                        no.row = 1,                      #Sample size
  #                                                        d = length(concentration_u[i,]), #Number of categories (same as length of concentration vector)
  #                                                        alpha = concentration_u[i,],     #Concentration vector
  #                                                        beta = 1                         #Common shape parameter
  #                                                             )
}
```

Check rowsums
```{r}
identical(rowSums(n_u), Nu)
```

#### Stimulated

Generate stimulated count values for each subject
```{r}
#Generate total counts
Ns = ceiling(
      rnorm(mean = e.total.mu, 
            sd = e.total.sd,
            n = I)
)

Ns.subset = ceiling(
              rnorm(mean = e.subset.mu, 
                    sd = e.subset.sd,
                    n = I
              ) 
)
```

Loop over each sample and do two things:

  1. Create a vector of length K - 1 (not K) of concentration parameters drawn from a lognormal distribution with an unstimulated-specific log-mean.
  2. Using the vector of concentration parameters, draw a vector of counts of length K - 1 (not K) from a Dirichlet-Multinomial distribution and fill the first K - 1 columns of the ith row of n_u

**Notes:** 

  - Leave the last column as 0
  - Concentration is shifted by ps.logmu.diff if gamma = 1
  - Use Nu instead of Nu.subset because now Kth category included in DM-generated data

```{r}
concentration_s = matrix(NA,
                         nrow = I,
                         ncol = K
                  )

for (i in 1:I) {
  concentration_s[i, 1:K1] = rlnorm(n = K1, 
                 meanlog = (pu.logmu + ps.logmu.diff * gamma.m[i, ]), 
                 sdlog = p.logsd
  )
  
  concentration_s[i, K] = rlnorm(n = 1,
                                meanlog = pu.logmu*3,
                                sdlog = p.logsd
  )
  #HMP version
  n_s[i,1:K] = t(matrix(HMP::Dirichlet.multinomial(Nrs = Ns[i], shape = concentration_s[i,])))
  
  # #MultiRNG version
  # n_s[i,1:K] = MultiRNG::draw.dirichlet.multinomial(N = Ns[i],                #Total counts
  #                                                    no.row = 1,                      #Sample size
  #                                                    d = length(concentration_s[i,]), #Number of categories (same as length of concentration vector)
  #                                                    alpha = concentration_s[i,],     #Concentration vector
  #                                                    beta = 1                         #Common shape parameter
  #                                                             )
}

```

Check rowsums
```{r}
identical(rowSums(n_s), Ns)
```

Check that rowsums are similar for sample sample between unstim and stim
```{r}
rowSums(n_u)[1:10]
rowSums(n_s)[1:10]
```

### Run COMPASScovariate

Create a version of X without an intercept column
```{r}
X.mod = X[,-1]
```

```{r message=TRUE, warning=FALSE}
sim2.fit = .COMPASS.covariate(n_s = n_s, 
                             n_u = n_u,
                             X = X.mod,
                             iterations = iter, 
                             replication = rep)
```

### Compare Estimates to True

Create a custom function for calculating the mode
```{r}
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
```

#### Gamma

```{r}
sim2.fit$mean_gamma[1:4,]
```

```{r}
sim2.fit.gamma <- apply(sim2.fit$gamma, MARGIN = c(1,2), FUN = function(x) getmode(x))

sim2.fit.gamma[1:4,]
```


```{r}
gamma.m[1:4,]
```

```{r}
n_u[1:4,]
n_s[1:4,]
```

##### Percentage Correct

```{r}
print(
  paste(
    "Percent gamma correct (excluding Kth subset):",
    round(
      sum(gamma.m == sim2.fit.gamma[,-ncol(sim2.fit.gamma)]) /
        (dim(gamma.m)[1] * dim(gamma.m)[2]) * 100,
      digits = 2
    )
  )
)
```

#### Betas

```{r}
sim2.fit.beta.median <- apply(sim2.fit$beta, MARGIN = c(1,2), FUN = function(x) median(x))

sim2.fit.beta.median
```

```{r}
beta.true.final
```

##### 95% Interval Coverage

Find 95% quantile intervals for each beta
```{r}
lower.95 <- apply(sim2.fit$beta, MARGIN = c(1,2), FUN = function(x) quantile(x,
                                                                 prob = 0.025
                                                                 )
      )

upper.95 <- apply(sim2.fit$beta, MARGIN = c(1,2), FUN = function(x) quantile(x,
                                                                 prob = 0.975
                                                                 )
      )
```

Find percentage of true values outside of 95% quantile interval
```{r}
beta.coverage <- beta.true.final >= lower.95 & beta.true.final <= upper.95

print(
  paste(
    "Percent true betas within 95% quantile interval:",
    round(
      sum(
        beta.coverage
      ) / length(beta.true.final) * 100,
      2
    )
  )
)
```

##### Plot Posteriors Not Achieving Coverage

```{r}
#Loop over rows of beta.diff and find row,col of estimates that fail to achieve coverage
beta.no.cover.coord <- list(NULL)
for (i in 1:nrow(beta.coverage)) {
  if(length(which(beta.coverage[i,] == FALSE)) > 0) {
    #Paste the row name (i) with the indices of the columns for the current row
    beta.no.cover.coord[[i]] <- paste(i, which(beta.coverage[i,] == FALSE), sep = ", ")
  } else {
    beta.no.cover.coord[[i]] <- NULL
  }
}

beta.no.cover.coord.df <- do.call('c', beta.no.cover.coord)
```

Extract and plot posterior samples for flagged coefs
```{r}
lapply(1:length(beta.no.cover.coord.df), FUN = function(i) {
  #Extract the posterior samples for the current beta
  post <- data.frame(samples = eval(parse(text = paste0("sim2.fit$beta",
                           "[",
                           beta.no.cover.coord.df[i],
                           ",]"
                           )
                                         )
                                   )
       )
  
  #Extract the true beta value
  true.beta <- eval(parse(text = paste0("beta.true.final",
                         "[",
                         beta.no.cover.coord.df[i],
                         "]"
                                   )
                     )
               )
  
  #Make the plot
  ggplot(data = post,
         aes(x = samples)
    )+
    theme_bw()+
    geom_vline(xintercept = 0,
               size = 0.6,
               color = "dark gray"
    )+
    geom_hline(yintercept = 0,
               size = 0.6,
               color = "dark gray"
    )+
    ggdist::stat_halfeye(
      .width = c(0.95, 0.8),
      point_interval = "median_qi"    #Use median for point est
    )+
    geom_vline(xintercept = true.beta,
               color = "blue"
    )+
    xlab("Posterior sample")+
    labs(title = paste0("Cluster: ", 
                stringr::str_split(beta.no.cover.coord.df[i],
                                   pattern = ", ",
                                   simplify = TRUE
                )[1],
                ", Covariate: ",
                stringr::str_split(beta.no.cover.coord.df[i],
                                   pattern = ", ",
                                   simplify = TRUE
                )[2]
                )
    )
})
```

##### Plot Posteriors for Estimates far from True Value

A cutoff value for the diff between true and estimated beta to decide whether to plot the corresponding posterior
```{r}
beta.diff.cutoff <- 0.1
```

Find top values of absolute difference between estimate and true beta
```{r}
beta.diff <- abs(beta.true.final - sim2.fit.beta.median)

#Loop over rows of beta.diff and find row,col of "bad" values where diff is high
beta.poor.coord <- list(NULL)
for (i in 1:nrow(beta.diff)) {
  if(length(which(beta.diff[i,] > beta.diff.cutoff)) > 0) {
    #Paste the row name (i) with the indices of the columns for the current row
    beta.poor.coord[[i]] <- paste(i, which(beta.diff[i,] > beta.diff.cutoff), sep = ", ")
  } else {
    beta.poor.coord[[i]] <- NULL
  }
}

beta.poor.coord.df <- do.call('c', beta.poor.coord)
```

Extract and plot posterior samples for flagged coefs
```{r}
lapply(1:length(beta.poor.coord.df), FUN = function(i) {
  #Extract the posterior samples for the current beta
  post <- data.frame(samples = eval(parse(text = paste0("sim2.fit$beta",
                           "[",
                           beta.poor.coord.df[i],
                           ",]"
                           )
                                         )
                                   )
       )
  
  #Extract the true beta value
  true.beta <- eval(parse(text = paste0("beta.true.final",
                         "[",
                         beta.poor.coord.df[i],
                         "]"
                                   )
                     )
               )
  
  #Make the plot
  ggplot(data = post,
         aes(x = samples)
    )+
    theme_bw()+
    geom_vline(xintercept = 0,
               size = 0.6,
               color = "dark gray"
    )+
    geom_hline(yintercept = 0,
               size = 0.6,
               color = "dark gray"
    )+
    ggdist::stat_halfeye(
      .width = c(0.95, 0.8),
      point_interval = "median_qi"    #Use median for point est
    )+
    geom_vline(xintercept = true.beta,
               color = "blue"
    )+
    xlab("Posterior sample")+
    labs(title = paste0("Cluster: ", 
                stringr::str_split(beta.poor.coord.df[i],
                                   pattern = ", ",
                                   simplify = TRUE
                )[1],
                ", Covariate: ",
                stringr::str_split(beta.poor.coord.df[i],
                                   pattern = ", ",
                                   simplify = TRUE
                )[2]
                )
    )
})
```



## Method 3: Lynn's Original Method

### True Regression Coefs

```{r}
X <- matrix(rnorm(I*p), ncol = p, nrow = I) # covariates
mu_true = rnorm(K, 0, 1) # the true intercept
alpha_true = matrix(rbinom(p*K1, 1, 0.7), nrow = K1) # variable selection indicators

B_true = matrix(rnorm(K1*p, 0, 5), nrow = K1)
Balpha = B_true*alpha_true # coefficients with zeros
```

### True Gamma Matrix

```{r}
gamma <- matrix(0, nrow = I, ncol =  K) ## 0/1 response

#Loop over samples and create a gamma[i,k] via linear predictor from coefficients
for(k in 1:K1){
  z = mu_true[k] + X%*%(Balpha[k,]) # linear combination with a bias
  pr = 1/(1+exp(-z))   # pass through an inv-logit function
  gamma[,k] = unlist(lapply(pr, function(x) rbinom(1,1, x)))
}

gamma[,K] = rbinom(I,1,0.5) #Kth category has a 50/50 chance of being 1
gamma_true = gamma
```

### True Counts

```{r}
pu_true = MCMCpack::rdirichlet(I, rep(10, K1)) #True unstim cluster props with concentration = 10
n_u = array(0, dim = c(I, K)) #Container for k unstim cluster counts with 0s
Nu = ceiling(rnorm(I,20000, 10)) #Make the total events per sample 20,000 on average
#For each sample, generate counts in each k1 clusters, leaving the kth cluster as 0, with event count = 5000
for(i in 1:I){
  n_u[i,1:K1] = rmultinom(1, 5000, pu_true[i,])
}
#Make the kth cluster be the remaining events for the sample
n_u[,K] = Nu - rowSums(n_u)
ps_true = pu_true

#Loop over samples
for(i in 1:I){
  l1 = which(gamma[i,1:K1]==1)
  l0 = which(gamma[i,1:K1]==0)
  ps_true[i,l1] = MCMCpack::rdirichlet(1, rep(20, length(l1)))*sum(pu_true[i,l1]) #For the clusters where gamma = 1, then change the prop by increasing concentration to 20, and normalize these to maintain 1
}

n_s = array(0, dim = c(I, K))
Ns = ceiling(rnorm(I,30000, 10))
for(i in 1:I){
  n_s[i,1:K1] = rmultinom(1, 6000, ps_true[i,])
}
n_s[,K] = Ns - rowSums(n_s)
```

### Run COMPASScovariate

```{r message=FALSE, warning=FALSE}
sim2.fit = .COMPASS.covariate(n_s = n_s, 
                             n_u = n_u,
                             X = X,
                             iterations = iter, 
                             replication = rep)
```

### Compare Estimates to True

#### Gamma

```{r}
sim2.fit$mean_gamma[1:2,]
```

```{r}
apply(sim2.fit$gamma, MARGIN = c(1,2), FUN = function(x) getmode(x))[1:2,]
```

```{r}
gamma_true[1:2,]
```

```{r}
n_u[1:2,]
n_s[1:2,]
```
